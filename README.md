This code is written to implement Seq2Seq attention model for Machine Translation task for Parallel Hindi-English Corpora.

I have created joint\_Corpus.txt to do test-train split from main corpus, which is later used in both models and their inference codes.

1.  a)  File name for plain encoder decoder training model -
        Seq2Seq\_Keras\_Training\_Assignment2\_v1.ipynb

	b)  File name for encoder-decoder Inference model -
		Seq2Seq\_Keras\_Inference\_Assignment2.ipynb
		
	c)  File name for encoder-decoder with Attention training model -
		Seq2Seq\_attention\_training\_Assignment2.ipynb
		
	d)  File name for encoder-decoder with Attention Inference model -
		Seq2Seq\_AttentionB\_Keras\_Inf\_Assignment2.ipynb

2.  a)  Test prediction of 1000 test sentence using plain
        encoder-decoder model - test\_prediction\_without\_attention

	b)  Test prediction of 1000 test sentence using encoder-decoder with
		with attention model - test\_prediction

3.  Word document of report is attached which shows 20 sentence predicted translation
4.  Trained Model is saved and placed at google drive.

	link - https://drive.google.com/drive/folders/1AFU4PhVza6A8H\_YTZtmnlKcfJar8f2JK?usp=sharing

	a)  Seq2Seq\_assignment2\_v1.h5
	b)  Seq2Seq\_attentionB\_assignment2\_v1.h5

